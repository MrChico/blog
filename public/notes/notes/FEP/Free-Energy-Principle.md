# Free Energy Principle (Karl Friston)

Interesting Papers:
* [Active inference and learning](http://www.fil.ion.ucl.ac.uk/~karl/Active%20inference%20and%20learning.pdf)
* [Active inference: A Process Theory](http://www.fil.ion.ucl.ac.uk/~karl/Active%20Inference%20A%20Process%20Theory.pdf)
* [Hierarchical Models in the Brain](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000211)

Articles:
* [The free-energy principle: a rough guide to the brain?](http://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20-%20a%20rough%20guide%20to%20the%20brain.pdf)
[Markov Blanket](https://en.wikipedia.org/wiki/Markov_blanket)

* Videos:

# Videos

[Karl Friston: Active inference and artificial curiosity](https://www.youtube.com/watch?v=Y1egnoCWgUg)

# Owl metaphor

Hungry. 
* look for prey. 
* Resolve uncertainty of location the prey before explotinve behavior.

<img class="article-image" src="/images/notes/notes/Free-energy-principle/slide.png" title="title"> 


# Article
"This article reviews a free-energy formulation that
advances Helmholtzâ€™s agenda to find principles of brain
function based on conservation laws and neuronal
energy."


> However, an agent can avoid surprising exchanges
> with the world if it minimises its free-energy because free-
> energy is always bigger than surprise.


## Glossary from article
<div class="glossary">

<ul id="glossoray-item-list">
<li>
[Kullback-Leibler divergence]: information divergence, information gain, cross or relative entropy is a non-commutative measure of the difference between two probability distributions.  <br><br>

<li> 
[Bayesian surprise]: a measure of salience based on the divergence between the recognition and prior densities. It measures the information in the data that can be recognised. 
</li>

<li>
<strong>Conditional density</strong> : or posterior density is the probability distribution of causes or model parameters, given some data; i.e., a probabilistic mapping from observed data to causes. 
</li>

<li>Empirical priors: priors that are induced by hierarchical models; they provide
constraints on the recognition density is the usual way but depend on the data
</li>

<li>
Entropy: the average surprise of outcomes sampled from a probability
distribution or density. A density with low entropy means, on average, the
outcome is relatively predictable.
</li>


<li><a href="https://en.wikipedia.org/wiki/Ergodic_theory">Ergodic </a></li>
<div class="card">
<img align="right" src="https://upload.wikimedia.org/wikipedia/commons/f/f7/Hamiltonian_flow_classical.gif" alt="Flowers in Chania"> 
<h1> Ergodic Process</h1>

<a href="https://en.wikipedia.org/wiki/Ergodic_theory">Ergodic </a>

a process is ergodic if its long term time-average converges to its ensemble average. Ergodic processes that evolve for a long time forget their initial states.
</div>


<li>
Free-energy: an information theory measure that bounds (is greater than) the surprise on sampling some data, given a generative model.
</li>

<li>
Generalised coordinates: of motion cover the value of a variable, its motion, acceleration, jerk and higher orders of motion. A point in generalised coordinates corresponds to a path or trajectory over time.<br> 
</li>

<li>Generative model: or forward model is a probabilistic mapping from causes to observed consequences (data). It is usually specified in terms of the likelihood of getting some data given their causes (parameters of a model) and priors on the parameters
</li>

<li>Gradient descent: an optimisation scheme that finds a minimum of a function by changing its arguments in proportion to the negative of the gradient of the function at the current value.
</li>

<li>
Helmholtz machine: device or scheme that uses a generative model to furnish a recognition density. They learn hidden structure in data by optimising the parameters of generative models.  
</li>

<li>
Prior: the probability distribution or density on the causes of data that encode beliefs about those causes prior to observing the data.<br> <br>
</li>

<li>
[Recognition density](): or approximating conditional density is an approximate probability distribution of the causes of data. It is the product of inference or inverting a generative model.  Stochastic: the successive states of stochastic processes are governed by random effects.
</li>

<li>
[Sufficient statistics](): quantities which are sufficient to parameterise a probability density (e.g., mean and covariance of a Gaussian density).  Surprise: or self-information is the negative log-probability of an outcome. An improbable outcome is therefore surprising. 
</li>
</li>
</ul>
</div>
